<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WebCodecs Opus Audio Test</title>
    <!-- 1. Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* 2. Use Inter font */
        body {
            font-family: 'Inter', sans-serif;
        }
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap');
    </style>
</head>
<body class="bg-gray-100 text-gray-900 flex items-center justify-center min-h-screen">

    <div class="max-w-2xl w-full bg-white rounded-xl shadow-lg p-8 space-y-6">
        <h1 class="text-3xl font-bold text-center text-blue-600">WebCodecs AudioEncoder/Decoder Test</h1>
        <p class="text-center text-gray-600">Test Opus encoding with a custom frame duration.</p>

        <!-- 3. Controls -->
        <div class="flex flex-col sm:flex-row gap-4 items-center justify-center">
            <div class="flex-1 w-full">
                <label for="frameDuration" class="block text-sm font-medium text-gray-700 mb-1">Opus Frame Duration (μs)</label>
                <input type="number" id="frameDuration" value="7500" class="w-full px-4 py-2 border border-gray-300 rounded-lg shadow-sm focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-blue-500">
                <p class="text-xs text-gray-500 mt-1">e.g., 2500 (2.5ms), 5000 (5ms), 7500 (7.5ms), 10000 (10ms)</p>
            </div>
            <button id="startButton" class="w-full sm:w-auto px-6 py-3 bg-blue-600 text-white font-medium rounded-lg shadow-md hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 transition-all duration-150">Start Test</button>
            <button id="stopButton" class="w-full sm:w-auto px-6 py-3 bg-red-600 text-white font-medium rounded-lg shadow-md hover:bg-red-700 focus:outline-none focus:ring-2 focus:ring-red-500 focus:ring-offset-2 transition-all duration-150" disabled>Stop Test</button>
        </div>

        <!-- 4. Status -->
        <div id="status" class="text-center font-medium text-gray-700 bg-gray-100 p-4 rounded-lg">
            Click "Start Test" to begin.
        </div>

        <!-- 5. Audio Players -->
        <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
            <div class="bg-gray-50 p-4 rounded-lg shadow-inner">
                <h3 class="text-lg font-semibold mb-2">Original Audio (Live Mic)</h3>
                <audio id="originalPlayer" controls class="w-full"></audio>
            </div>
            <div class="bg-gray-50 p-4 rounded-lg shadow-inner">
                <h3 class="text-lg font-semibold mb-2">Decoded Audio (After Stop)</h3>
                <audio id="decodedPlayer" controls class="w-full"></audio>
            </div>
        </div>
    </div>

    <script>
        // --- Globals ---
        let mediaStream = null;
        let audioContext = null;
        let scriptNode = null;
        let mediaStreamSource = null;
        let audioEncoder = null;
        let audioDecoder = null;

        let decodedFrames = [];
        let encoderSampleRate = 48000; // Default, will be updated
        let encoderChannelCount = 1; // Default, will be updated

        // --- DOM Elements ---
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const durationInput = document.getElementById('frameDuration');
        const statusEl = document.getElementById('status');
        const originalPlayer = document.getElementById('originalPlayer');
        const decodedPlayer = document.getElementById('decodedPlayer');

        // --- Event Listeners ---
        startButton.addEventListener('click', startTest);
        stopButton.addEventListener('click', stopTest);

        // --- Core Functions ---

        async function startTest() {
            if (!window.AudioEncoder || !window.AudioDecoder) {
                updateStatus('Error: WebCodecs API not supported in this browser.', true);
                return;
            }

            startButton.disabled = true;
            stopButton.disabled = false;
            decodedPlayer.src = '';
            decodedFrames = [];
            updateStatus('Requesting microphone permission...');

            try {
                // 1. Get Microphone Stream
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 48000, // Request a common rate for Opus
                        channelCount: 1,
                    }
                });
                
                // 2. Play Original Audio
                originalPlayer.srcObject = mediaStream;
                originalPlayer.play();
                originalPlayer.muted = true; // Mute the <audio> element, we will play via Web Audio

                updateStatus('Microphone connected. Initializing codecs...');

                // 3. Setup Web Audio Context
                audioContext = new AudioContext();
                encoderSampleRate = audioContext.sampleRate;
                mediaStreamSource = audioContext.createMediaStreamSource(mediaStream);
                
                // We use 1 input channel, 1 output channel
                encoderChannelCount = 1; 
                
                // Buffer size for ScriptProcessorNode (must be power of 2)
                const bufferSize = 4096; 
                scriptNode = audioContext.createScriptProcessor(bufferSize, encoderChannelCount, encoderChannelCount);
                
                // 4. Setup Codecs
                const frameDuration = parseInt(durationInput.value, 10);
                if (isNaN(frameDuration) || frameDuration <= 0) {
                    updateStatus('Invalid frame duration.', true);
                    stopTest();
                    return;
                }

                initEncoder(frameDuration);
                initDecoder();
                
                // 5. Connect Audio Graph
                scriptNode.onaudioprocess = handleAudioProcess;
                mediaStreamSource.connect(scriptNode);
                scriptNode.connect(audioContext.destination); // Play original audio live

                updateStatus(`Test started. Encoding with Opus frame duration: ${frameDuration}μs`, false);

            } catch (err) {
                console.error('Error starting test:', err);
                updateStatus(`Error: ${err.message}`, true);
                stopButton.click(); // Run cleanup
            }
        }

        function stopTest() {
            startButton.disabled = false;
            stopButton.disabled = true;
            updateStatus('Test stopping...');

            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            
            if (originalPlayer.srcObject) {
                originalPlayer.srcObject = null;
            }

            if (scriptNode) {
                scriptNode.disconnect();
                scriptNode.onaudioprocess = null;
                scriptNode = null;
            }

            if (mediaStreamSource) {
                mediaStreamSource.disconnect();
                mediaStreamSource = null;
            }

            if (audioEncoder && audioEncoder.state !== 'closed') {
                audioEncoder.close();
                audioEncoder = null;
            }

            if (audioDecoder && audioDecoder.state !== 'closed') {
                audioDecoder.close();
                audioDecoder = null;
            }
            
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.close();
                audioContext = null;
            }
            
            updateStatus('Processing decoded audio...');
            buildAndPlayDecodedWav();
        }
        
        // --- WebCodecs ---

        function initEncoder(frameDuration) {
            audioEncoder = new AudioEncoder({
                output: handleEncodedChunk,
                error: (err) => updateStatus(`Encoder error: ${err.message}`, true)
            });

            audioEncoder.configure({
                codec: 'opus',
                sampleRate: encoderSampleRate,
                numberOfChannels: encoderChannelCount,
                opus: {
                    frameDuration: frameDuration, 
                    // complexity: 10, // Optional: 0-10, 10 is highest quality/complexity
                    // bitrate: 64000, // Optional: e.g., 64000 (64kbps)
                }
            });
        }

        function initDecoder() {
            audioDecoder = new AudioDecoder({
                output: handleDecodedFrame,
                error: (err) => updateStatus(`Decoder error: ${err.message}`, true)
            });

            audioDecoder.configure({
                codec: 'opus',
                sampleRate: encoderSampleRate,
                numberOfChannels: encoderChannelCount,
            });
        }
        
        function handleAudioProcess(event) {
            if (!audioEncoder || audioEncoder.state !== 'configured') return;

            const inputBuffer = event.inputBuffer;
            const outputBuffer = event.outputBuffer;

            // Pass audio through for live playback
            for (let channel = 0; channel < outputBuffer.numberOfChannels; channel++) {
                const inputData = inputBuffer.getChannelData(channel);
                const outputData = outputBuffer.getChannelData(channel);
                outputData.set(inputData);
            }

            // --- Create AudioData ---
            // We must clone the data buffer because the ScriptProcessor reuses it
            const inputData = inputBuffer.getChannelData(0);
            const dataClone = inputData.buffer.slice(0);

            const audioData = new AudioData({
                format: 'f32-planar', // 'f32-planar' is standard for Web Audio
                sampleRate: inputBuffer.sampleRate,
                numberOfFrames: inputBuffer.length,
                numberOfChannels: inputBuffer.numberOfChannels,
                timestamp: event.playbackTime * 1_000_000, // timestamp in microseconds
                data: dataClone
            });

            try {
                audioEncoder.encode(audioData);
                // audioData is now owned by the encoder, no need to close
            } catch (err) {
                console.error('Encoding error:', err);
                audioData.close(); // Close it if encoding failed
            }
        }
        
        function handleEncodedChunk(chunk, metadata) {
            if (audioDecoder && audioDecoder.state === 'configured') {
                try {
                    audioDecoder.decode(chunk);
                } catch (err) {
                    console.error('Decoding error:', err);
                }
            }
        }

        function handleDecodedFrame(frame) {
            // We must clone the frame's data because the frame will be recycled
            const buffer = new ArrayBuffer(frame.allocationSize({planeIndex: 0}));
            frame.copyTo(buffer, {planeIndex: 0});
            
            decodedFrames.push({
                buffer: buffer,
                sampleRate: frame.sampleRate,
                numberOfFrames: frame.numberOfFrames,
                numberOfChannels: frame.numberOfChannels,
                format: frame.format
            });
            
            frame.close();
        }
        
        // --- UI & Helpers ---

        function updateStatus(message, isError = false) {
            statusEl.textContent = message;
            if (isError) {
                statusEl.classList.remove('bg-gray-100', 'text-gray-700');
                statusEl.classList.add('bg-red-100', 'text-red-700');
            } else {
                statusEl.classList.remove('bg-red-100', 'text-red-700');
                statusEl.classList.add('bg-gray-100', 'text-gray-700');
            }
        }

        function buildAndPlayDecodedWav() {
            if (decodedFrames.length === 0) {
                updateStatus('Test complete. No audio was decoded.', true);
                return;
            }

            try {
                // 1. Stitch all decoded Float32 buffers together
                const totalSamples = decodedFrames.reduce((acc, frame) => acc + frame.numberOfFrames, 0);
                const sampleRate = decodedFrames[0].sampleRate;
                const channelCount = decodedFrames[0].numberOfChannels;
                
                const audioBuffer = new AudioBuffer({
                    length: totalSamples,
                    sampleRate: sampleRate,
                    numberOfChannels: channelCount,
                });

                let offset = 0;
                for (const frame of decodedFrames) {
                    // Frame data is Float32Array
                    const frameData = new Float32Array(frame.buffer);
                    audioBuffer.getChannelData(0).set(frameData, offset);
                    // Add logic for multi-channel if needed
                    offset += frame.numberOfFrames;
                }
                
                // 2. Convert AudioBuffer to WAV
                const wavBlob = bufferToWave(audioBuffer);
                const wavUrl = URL.createObjectURL(wavBlob);
                
                // 3. Play
                decodedPlayer.src = wavUrl;
                decodedPlayer.play();
                updateStatus('Test complete. Playing decoded audio.');

            } catch (err) {
                console.error('Error building WAV:', err);
                updateStatus(`Error building decoded audio: ${err.message}`, true);
            }
            
            // Clear buffer
            decodedFrames = [];
        }


        /**
         * Converts an AudioBuffer to a WAV file (Blob).
         * @param {AudioBuffer} abuffer The AudioBuffer to convert.
         * @param {boolean} [asFloat=false] Whether to use 32-bit float (true) or 16-bit int (false).
         * @returns {Blob} A Blob representing the WAV file.
         */
        function bufferToWave(abuffer, asFloat = false) {
            let numOfChan = abuffer.numberOfChannels;
            let length = abuffer.length * numOfChan * (asFloat ? 4 : 2) + 44;
            let buffer = new ArrayBuffer(length);
            let view = new DataView(buffer);
            let channels = [], i, sample;
            let offset = 0;
            let pos = 0;

            // Helper function
            let writeString = (s) => {
                for (i = 0; i < s.length; i++) {
                    view.setUint8(pos++, s.charCodeAt(i));
                }
            };
            
            let floatTo16BitPCM = (output, offset, input) => {
                for (let i = 0; i < input.length; i++, offset += 2) {
                    let s = Math.max(-1, Math.min(1, input[i]));
                    output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
                }
            };
            
            let writeFloat32 = (output, offset, input) => {
                for (let i = 0; i < input.length; i++, offset += 4) {
                    output.setFloat32(offset, input[i], true);
                }
            };

            // RIFF chunk descriptor
            writeString('RIFF');
            view.setUint32(pos, 36 + abuffer.length * numOfChan * (asFloat ? 4 : 2), true); pos += 4;
            writeString('WAVE');

            // FMT sub-chunk
            writeString('fmt ');
            view.setUint32(pos, 16, true); pos += 4; // Subchunk1Size
            view.setUint16(pos, asFloat ? 3 : 1, true); pos += 2; // AudioFormat
            view.setUint16(pos, numOfChan, true); pos += 2; // NumChannels
            view.setUint32(pos, abuffer.sampleRate, true); pos += 4; // SampleRate
            view.setUint32(pos, abuffer.sampleRate * numOfChan * (asFloat ? 4 : 2), true); pos += 4; // ByteRate
            view.setUint16(pos, numOfChan * (asFloat ? 4 : 2), true); pos += 2; // BlockAlign
            view.setUint16(pos, (asFloat ? 32 : 16), true); pos += 2; // BitsPerSample

            // DATA sub-chunk
            writeString('data');
            view.setUint32(pos, abuffer.length * numOfChan * (asFloat ? 4 : 2), true); pos += 4; // Subchunk2Size

            // Write audio data
            for (i = 0; i < abuffer.numberOfChannels; i++) {
                channels.push(abuffer.getChannelData(i));
            }

            let dataWriter = asFloat ? writeFloat32 : floatTo16BitPCM;
            offset = pos;
            
            // Interleave channels
            for(i = 0; i < abuffer.length; i++) {
                for(let chan = 0; chan < numOfChan; chan++) {
                    let sample = channels[chan][i];
                    if (asFloat) {
                        view.setFloat32(offset, sample, true);
                        offset += 4;
                    } else {
                        // 16-bit PCM
                        sample = Math.max(-1, Math.min(1, sample));
                        view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
                        offset += 2;
                    }
                }
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }

    </script>
</body>
</html>
